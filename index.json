[{"content":"VSP upgoing/downgoing case Coming soon\nNo specific task case Coming soon\n","permalink":"https://geolxb.github.io/openWFS/tutorial/step1/","summary":"VSP upgoing/downgoing case Coming soon\nNo specific task case Coming soon","title":"Step 1: creat training datasets"},{"content":"Coming soon\n","permalink":"https://geolxb.github.io/openWFS/tutorial/step2/","summary":"Coming soon","title":"Step 2: merge datasets for training"},{"content":"Coming soon\n","permalink":"https://geolxb.github.io/openWFS/tutorial/step3/","summary":"Coming soon","title":"Step 3: training model"},{"content":"Coming soon\n","permalink":"https://geolxb.github.io/openWFS/tutorial/optimization_models/","summary":"Coming soon","title":"Optimization model"},{"content":"This project aims to provide an efficient and accurate deep learning algorithm for seismic data wavefield separation. The algorithm functions similarly to F-K filtering and Radon transform filtering and is applicable to various scenarios, including VSP up- and down-going wave separation, DAS-VSP single-component P/S separation based on apparent velocity differences, multiple-wave attenuation in CMP and CRP gathers, and linear noise attenuation. This method is suitable for tasks where the wavefields to be separated have significant apparent velocity differences.\nThe project is built on the PyTorch framework and includes training datasets and pre-trained models. It is open-sourced under the CC BY-SA 4.0 License, allowing use, sharing, adaptation, and redistribution with proper attribution to the author. The code is provided \u0026ldquo;as-is,\u0026rdquo; without any warranties or liability.\nThis project will continue to improve and expand the datasets. If you have any seismic data that you are willing to share for this purpose, please feel free to contact me.\nAuthor: Xiaobin Li\nContact me: lixb9767@163.com\n","permalink":"https://geolxb.github.io/openWFS/about/","summary":"This project aims to provide an efficient and accurate deep learning algorithm for seismic data wavefield separation. The algorithm functions similarly to F-K filtering and Radon transform filtering and is applicable to various scenarios, including VSP up- and down-going wave separation, DAS-VSP single-component P/S separation based on apparent velocity differences, multiple-wave attenuation in CMP and CRP gathers, and linear noise attenuation. This method is suitable for tasks where the wavefields to be separated have significant apparent velocity differences.","title":""},{"content":"core python functions：sep_function.py separate_one(data, model, boundary=0, time_interp:int=1, sampling_x:int=1, segment_x:int=1024, segment_overlap:int=50, iter:int=1, method=\u0026#34;up\u0026#34;): data: input seismic data: A 2-dimentional numpy.array model: The trained deep learning model boundary: Boundary slope or boundary T-D curves time_interp: Time interpolation sampling_x: Spatial downsampling rate segment_x: Segment length in the spatial direction segment_overlap: An overlap is set between the groups iter: The number of repeated predictions method: \u0026#34;up\u0026#34; \u0026#34;down\u0026#34; \u0026#34;middle\u0026#34; \u0026#34;outside\u0026#34; \u0026#34;median\u0026#34; separate_gathers(data, model, boundary=0, time_interp:int=1, sampling_x:int=1, segment_x:int=1024, segment_overlap:int=50, iter:int=1, method=\u0026#34;up\u0026#34;): data: 3-dimentional seismic data, 0-axis: list or numpy; 1- and 2-axis: numpy dim0: gather; dim1: time; dim2: trace Case 1: VSP upgoing and dwongoing The trained model, when used with default parameters, can effectively separate upgoing and downgoing waves.\nup = separate_gathers(data, model=model_up) down = data - up For DAS-VSP data with a high spatial sampling rate (e.g., 1 m or 0.5 m), the value of sampling_x can be appropriately increased.\nup = separate_gathers(data, model=model_up，sampling_x=2) down = data - up Example: Case 2: DAS-VSP P/S Shear and compressional wave separation in single-component DAS data can be achieved based on the the aparent velocity difference.\nup = sepf.separate_one(data, model=model_up) down = data-up up_s = sepf.separate_one(up, model=model_up,boundary=-2) up_p = up - up_s down_p = sepf.separate_one(down, model=model_up, boundary=2.5) down_s = down - down_p Example: Using manual picking of wave events (similar to median filtering).\ndown_p = separate_one(data, model=model_up, boundary=(T-D1,1,-1), method=\u0026quot;median\u0026quot;) down_s = separate_one(data-down_p, model=model_up, boundary=(T-D2,1,-1), method=\u0026quot;median\u0026quot;) up_p = separate_one(data-down_p-down_s, model=model_up, boundary=(T-D3,1,-1), method=\u0026quot;median\u0026quot;) up_s = separate_one(data-down_p-down_s-up_p, model=model_up, boundary=(T-D4,1,-1),method=\u0026quot;median\u0026quot;) residual = data-down_p-down_s-up_p-up_s Example: Case 3: CDP \u0026amp; CMP \u0026amp; CRP After NMO or migration, the effective signals typically display a horizontal events. Therefore, it is essential to remove high-angle signals, which may include multiples or linear noise.\nout = separate_gathers(data, model=model_up, boundary=(1,-1), time_interp=2, method=\u0026#34;middle\u0026#34;) Example: Case 4: Common mode noise The common-mode noise is primarily horizontal events. Since the DAS data has a high spatial resolution, sampling_x can be set to 2 to enhance the slope differences between effective signals and noise.\nout = separate_one(data, model=model_up, boundary=(1,-1), sampling_x=2, method=\u0026#34;outside\u0026#34;) Example: Case 5: Migration arc noise The migration algorithm may introduce arc-shaped noise. Since the data is in the depth domain, the parameter time_interp is essential for controlling the velocity range.\nout = separate_gathers(image, model=model_up, boundary=(T-D, 4,-4), time_interp=7, method=\u0026#34;median\u0026#34;) Example: ","permalink":"https://geolxb.github.io/openWFS/application/","summary":"core python functions：sep_function.py separate_one(data, model, boundary=0, time_interp:int=1, sampling_x:int=1, segment_x:int=1024, segment_overlap:int=50, iter:int=1, method=\u0026#34;up\u0026#34;): data: input seismic data: A 2-dimentional numpy.array model: The trained deep learning model boundary: Boundary slope or boundary T-D curves time_interp: Time interpolation sampling_x: Spatial downsampling rate segment_x: Segment length in the spatial direction segment_overlap: An overlap is set between the groups iter: The number of repeated predictions method: \u0026#34;up\u0026#34; \u0026#34;down\u0026#34; \u0026#34;middle\u0026#34; \u0026#34;outside\u0026#34; \u0026#34;median\u0026#34; separate_gathers(data, model, boundary=0, time_interp:int=1, sampling_x:int=1, segment_x:int=1024, segment_overlap:int=50, iter:int=1, method=\u0026#34;up\u0026#34;): data: 3-dimentional seismic data, 0-axis: list or numpy; 1- and 2-axis: numpy dim0: gather; dim1: time; dim2: trace Case 1: VSP upgoing and dwongoing The trained model, when used with default parameters, can effectively separate upgoing and downgoing waves.","title":"Application case"},{"content":"License(s): Creative Commons Attribution Share-Alike 4.0 (CC BY-SA 4.0)\n0 Introduction The datasets are stored in HDF5 files, with each file containing 20,000 upgoing and downgoing labels for a size of 128x128, or 40,000 labels for a size of 128x48. The dataset and code can be downloaded from Google Drive. In the HDF5 files, the patche order has been randomly shuffled. Using HDF5 VirtualLayout, a virtual file is generated to merge all sub-files for model training.\n1 Convolutional seismic datasets Number of label pairs Patch size Upgoing example Downgoing example Download 822448 128x128 Google Drive 2 Wave equation simulation 2.1 VSP Eqution：Acoustic wave； Ricker wavelet\nTime Sampling Rate: 1ms 2ms 4ms\nSpatial Sampling Rate: 1m 5m 10m 20m\nModel (Number of velocity models) Number of label pairs Patch size Upgoing examples Downgoing examples Download BP (2) 486200 128x128 Google Drive SEAM (2) 732774 128x128 Google Drive Marmousi (3) 1279434 128x128 Google Drive Sigsbee (3) 1333391 128x128 Google Drive Hess (2) 402082 128x128 Google Drive 2.2 Surface Eqution：Acoustic Wave and elastic wave； Ricker wavelet\nTime Sampling Rate: 1ms 2ms 4ms\nSpatial Sampling Rate: 2m 4m 5m 8m 10m 20m\nModel (Number of velocity models) Number of label pairs Patch size Upgoing examples Downgoing examples Download BP (3) 566326 128x128 Google Drive 1994 BP statics(1) 349245 128x128 Google Drive SEAM (4) 930695 128x128 Google Drive Marmousi (5) 877704 128x128 Google Drive Sigsbee (3, sigsbee2 is validation set) 668483 128x128 Google Drive Hess (3) 621952 128x128 Google Drive 3 Field data 3.1 Shot gathers (VSP) Data name Number of label pairs Patch size Upgoing example Downgoing example Download Cranfield 3DVS (validation set) 107823 128x48 Google Drive TR McMillen2 2DVSP 49078 128x48 Google Drive Private data(11 sets) 2813597 128*128 Not available Private data(2 sets) 296660 128*48 Not available Private data(4 sets) 1078603 128x128 Not available 3.2 Shot gathers (surface) Data name Number of label pairs Patch size Upgoing example Downgoing example Download USGS (validation set) 512765 128x48 Google Drive Private data(6 sets) 1826351 128x48 Not available Private data(2 sets) 84567 128x128 Not available 3.3 CDP/CRP/CMP Data name Number of label pairs Patch size Upgoing example Downgoing example Download Penobscot 3D (validation set) 666493 128x48 Google Drive Private data(9 sets) 4296183 128x48 Not available 3.4 Post-stack Data name Number of label pairs Patch size Upgoing example Downgoing example Download F3 Netherlands 110704 128x128 Google Drive Waihapa-3D 249652 128x128 Google Drive Opunake-3D 318780 128x128 Google Drive Parihaka-3D 241902 128x128 Google Drive Kerry-3D (validation set) 139924 128x128 Google Drive Tui-3D 385207 128x128 Google Drive Kahu-3D 290843 128x128 Google Drive Waka-3D 478230 128x128 Google Drive 4 Test datasets Data Methods Wavelet Time record Space record Shots 1 Margrave and Daley (2014) Ricker 2501x1s 281x5m 100 2 Elastic-z and cuting Puzirov 2501x1s 190x10m 175 3 Elastic-z\u0026amp;x and cuting Puzirov 2501x1s 190x10m 175 No. Examples Download 1 Google Drive 1-agc Google Drive 2 Google Drive 2-agc Google Drive 3 Google Drive 3-agc Google Drive ","permalink":"https://geolxb.github.io/openWFS/datasets/","summary":"License(s): Creative Commons Attribution Share-Alike 4.0 (CC BY-SA 4.0)\n0 Introduction The datasets are stored in HDF5 files, with each file containing 20,000 upgoing and downgoing labels for a size of 128x128, or 40,000 labels for a size of 128x48. The dataset and code can be downloaded from Google Drive. In the HDF5 files, the patche order has been randomly shuffled. Using HDF5 VirtualLayout, a virtual file is generated to merge all sub-files for model training.","title":"Datasets"}]